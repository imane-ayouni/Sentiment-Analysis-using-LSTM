{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b122d890",
   "metadata": {},
   "source": [
    "# Sentiment analysis using LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff190b4",
   "metadata": {},
   "source": [
    "## Topic\n",
    "\n",
    "In this notebook I will be working with and LSTM network to predict the sentiment in a sentence or paragraph. This is a supervised learning task seen as my sentences and paragraphs will be labeled. My dataset is in txt format, and my goal is to preprocess the textual data to make it machine learning ready, then feed it to LSTM cells that will take into account every word in the text to finally produce an answer (whether the input is positive or negative). So let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d30f05",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Process the textual data to make it machine learning ready\n",
    "- Predict the sentiment using a trained LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e6a921",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Importing libraries\n",
    "- The Dataset\n",
    "- Data pre-processing\n",
    "- Removing outliers\n",
    "- Padding features\n",
    "- Train/Validation/Test splits\n",
    "- Creating the data loaders\n",
    "- Defining the model\n",
    "- Training the model\n",
    "- Testing the model\n",
    "- Inference\n",
    "- Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6244a7cf",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "edbc3faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd41b171",
   "metadata": {},
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "389a2963",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/reviews.txt', 'r') as f:\n",
    "    messages = f.read()\n",
    "with open('data/labels.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b6520985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ff06dd37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive\\nnegative\\npositive\\nnegative\\npositive\\nnegat'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbb0a9e",
   "metadata": {},
   "source": [
    "So the dataset is two txt files one containing the sentences and paragraphs I need to classify and the second containing the labels corresponding to them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b777746a",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "66d7d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = messages.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d8405628",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join([x for x in messages if x not in punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eb8d4f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_splitted = text.split(\"\\n\")\n",
    "all_text = ' '.join(messages_splitted)\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cc54c836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42af77e",
   "metadata": {},
   "source": [
    "The first thing I did is to convert the text to lower characters then removed the punctuation. Then I splitted the text into individual paragraphs and then seperated each word on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c24a2431",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(words)\n",
    "vocab = sorted(count, key=count.get, reverse=True)\n",
    "vocab_to_int = {word:ii for ii,word in enumerate(vocab,1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5f318a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_int = []\n",
    "for message in messages_splitted:\n",
    "    messages_int.append([vocab_to_int[w] for w in message.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ada58107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21025,\n",
       " 308,\n",
       " 6,\n",
       " 3,\n",
       " 1050,\n",
       " 207,\n",
       " 8,\n",
       " 2138,\n",
       " 32,\n",
       " 1,\n",
       " 171,\n",
       " 57,\n",
       " 15,\n",
       " 49,\n",
       " 81,\n",
       " 5785,\n",
       " 44,\n",
       " 382,\n",
       " 110,\n",
       " 140,\n",
       " 15,\n",
       " 5194,\n",
       " 60,\n",
       " 154,\n",
       " 9,\n",
       " 1,\n",
       " 4975,\n",
       " 5852,\n",
       " 475,\n",
       " 71,\n",
       " 5,\n",
       " 260,\n",
       " 12,\n",
       " 21025,\n",
       " 308,\n",
       " 13,\n",
       " 1978,\n",
       " 6,\n",
       " 74,\n",
       " 2395,\n",
       " 5,\n",
       " 613,\n",
       " 73,\n",
       " 6,\n",
       " 5194,\n",
       " 1,\n",
       " 24103,\n",
       " 5,\n",
       " 1983,\n",
       " 10166,\n",
       " 1,\n",
       " 5786,\n",
       " 1499,\n",
       " 36,\n",
       " 51,\n",
       " 66,\n",
       " 204,\n",
       " 145,\n",
       " 67,\n",
       " 1199,\n",
       " 5194,\n",
       " 19869,\n",
       " 1,\n",
       " 37442,\n",
       " 4,\n",
       " 1,\n",
       " 221,\n",
       " 883,\n",
       " 31,\n",
       " 2988,\n",
       " 71,\n",
       " 4,\n",
       " 1,\n",
       " 5787,\n",
       " 10,\n",
       " 686,\n",
       " 2,\n",
       " 67,\n",
       " 1499,\n",
       " 54,\n",
       " 10,\n",
       " 216,\n",
       " 1,\n",
       " 383,\n",
       " 9,\n",
       " 62,\n",
       " 3,\n",
       " 1406,\n",
       " 3686,\n",
       " 783,\n",
       " 5,\n",
       " 3483,\n",
       " 180,\n",
       " 1,\n",
       " 382,\n",
       " 10,\n",
       " 1212,\n",
       " 13583,\n",
       " 32,\n",
       " 308,\n",
       " 3,\n",
       " 349,\n",
       " 341,\n",
       " 2913,\n",
       " 10,\n",
       " 143,\n",
       " 127,\n",
       " 5,\n",
       " 7690,\n",
       " 30,\n",
       " 4,\n",
       " 129,\n",
       " 5194,\n",
       " 1406,\n",
       " 2326,\n",
       " 5,\n",
       " 21025,\n",
       " 308,\n",
       " 10,\n",
       " 528,\n",
       " 12,\n",
       " 109,\n",
       " 1448,\n",
       " 4,\n",
       " 60,\n",
       " 543,\n",
       " 102,\n",
       " 12,\n",
       " 21025,\n",
       " 308,\n",
       " 6,\n",
       " 227,\n",
       " 4146,\n",
       " 48,\n",
       " 3,\n",
       " 2211,\n",
       " 12,\n",
       " 8,\n",
       " 215,\n",
       " 23]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_int[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e0067cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74072\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d8c45c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_splitted = labels.split(\"\\n\")\n",
    "labels_encoded = np.array([1 if label == \"positive\" else 0 for label in labels_splitted])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2c28a",
   "metadata": {},
   "source": [
    "Next I created a dictionary to hold unique words with  unique numbers each corresponding to a word, and transformed my texual reviews into lists of integers where each integer refers to a word. Then I encoded the labels two 1 for positive and 0 for negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1d8749",
   "metadata": {},
   "source": [
    "### Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f75ab840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shortest review  1\n",
      "Longest review 2514\n"
     ]
    }
   ],
   "source": [
    "messages_lens = Counter([len(x) for x in messages_int])\n",
    "print(\"shortest review \", messages_lens[0])\n",
    "print(\"Longest review\", max(messages_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "20632522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews 25001\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of reviews\", len(messages_int))\n",
    "non_zero = [ii for ii, r in enumerate(messages_int) if len(r) !=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e4284433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews after removing zero len reviews 25000\n"
     ]
    }
   ],
   "source": [
    "messages_int = [messages_int[ii] for ii in non_zero]\n",
    "labels_encoded = np.array([labels_encoded[ii] for ii in non_zero])\n",
    "print(\"Number of reviews after removing zero len reviews\", len(messages_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c46d42",
   "metadata": {},
   "source": [
    "The next thing I did is to remove the reviews that are 0 length which won't help the model in anything, and checked for the longest review which will help me decide on the sequence length later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8323438c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c37bbb3",
   "metadata": {},
   "source": [
    "### Padding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f1ab7dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(messages_int, seq_length):\n",
    "    features = np.zeros((len(messages_int), seq_length), dtype=int)\n",
    "    for i, row in enumerate(messages_int):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "55f9ab19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "seq_length = 200\n",
    "features = pad_features(messages_int, seq_length)\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "787cf80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "  21025   308     6     3  1050   207     8  2138    32     1   171    57\n",
      "     15    49    81  5785    44   382   110   140    15  5194    60   154\n",
      "      9     1  4975  5852   475    71     5   260    12 21025   308    13\n",
      "   1978     6    74  2395     5   613    73     6  5194     1 24103     5\n",
      "   1983 10166     1  5786  1499    36    51    66   204   145    67  1199\n",
      "   5194 19869     1 37442     4     1   221   883    31  2988    71     4\n",
      "      1  5787    10   686     2    67  1499    54    10   216     1   383\n",
      "      9    62     3  1406  3686   783     5  3483   180     1   382    10\n",
      "   1212 13583    32   308     3   349   341  2913    10   143   127     5\n",
      "   7690    30     4   129  5194  1406  2326     5 21025   308    10   528\n",
      "     12   109  1448     4    60   543   102    12 21025   308     6   227\n",
      "   4146    48     3  2211    12     8   215    23]]\n"
     ]
    }
   ],
   "source": [
    "print(features[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815371b6",
   "metadata": {},
   "source": [
    "In the above I chose a sequence length of 200, padded the reviews that are less than the sequence length long with 0s and trancated the reviews that are longer than the sequence length to be only 200 words long."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afb0bce",
   "metadata": {},
   "source": [
    "### Train/Validation/Test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "acea1017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  (20000, 200)\n",
      "Validation (2500, 200)\n",
      "Test  (2500, 200)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "split_idx = int(len(features)*split_frac)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = labels_encoded[:split_idx], labels_encoded[split_idx:]\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "print(\"Training \", train_x.shape)\n",
    "print(\"Validation\", val_x.shape)\n",
    "print(\"Test \", test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca0f869",
   "metadata": {},
   "source": [
    "### Creating the data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "22b56f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "val_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "batch_size = 30\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084014c9",
   "metadata": {},
   "source": [
    "Next I splitted my data into training, validation and testing sets, made tensor datasets out of those sets and created dataloaders for them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78006d2",
   "metadata": {},
   "source": [
    "### Definening the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7eca7a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers ,batch_first=True, dropout=drop_prob)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        em = self.embedding(x)\n",
    "        output, hidden = self.lstm(em, hidden)\n",
    "        output = output.reshape(-1, self.hidden_dim)\n",
    "        out = self.dropout(output)\n",
    "        out = self.fc(out)\n",
    "        out_sig = self.sig(out)\n",
    "        out_sig = out_sig.view(batch_size, -1)\n",
    "        out_sig = out_sig[:, -1]\n",
    "        return out_sig, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d444ea8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (embedding): Embedding(74073, 200)\n",
      "  (lstm): LSTM(200, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab_to_int)+1\n",
    "output_size = 1\n",
    "embedding_dim = 200\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "net = LSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8ddc9f",
   "metadata": {},
   "source": [
    "Here I defined my model, I chose an lstm with 2 layers, and added an embedding layer to map the words into my integers into continuous vectors, added a dropout layers to avoid overfitting and chose a Sigmoid activation because I have only two possible outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea6a3ab",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8ef17794",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "83bf0e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4... Step: 100... Loss: 0.635072... Val Loss: 0.672856\n",
      "Epoch: 1/4... Step: 200... Loss: 0.700416... Val Loss: 0.668398\n",
      "Epoch: 1/4... Step: 300... Loss: 0.637585... Val Loss: 0.660066\n",
      "Epoch: 1/4... Step: 400... Loss: 0.713489... Val Loss: 0.608056\n",
      "Epoch: 1/4... Step: 500... Loss: 0.810142... Val Loss: 0.606783\n",
      "Epoch: 1/4... Step: 600... Loss: 0.716677... Val Loss: 0.565104\n",
      "Epoch: 2/4... Step: 700... Loss: 0.694538... Val Loss: 0.535692\n",
      "Epoch: 2/4... Step: 800... Loss: 0.513789... Val Loss: 0.608465\n",
      "Epoch: 2/4... Step: 900... Loss: 0.337506... Val Loss: 0.501231\n",
      "Epoch: 2/4... Step: 1000... Loss: 0.314367... Val Loss: 0.498360\n",
      "Epoch: 2/4... Step: 1100... Loss: 0.503584... Val Loss: 0.486660\n",
      "Epoch: 2/4... Step: 1200... Loss: 0.486876... Val Loss: 0.519690\n",
      "Epoch: 2/4... Step: 1300... Loss: 0.287363... Val Loss: 0.433715\n",
      "Epoch: 3/4... Step: 1400... Loss: 0.271825... Val Loss: 0.461903\n",
      "Epoch: 3/4... Step: 1500... Loss: 0.424831... Val Loss: 0.459054\n",
      "Epoch: 3/4... Step: 1600... Loss: 0.499383... Val Loss: 0.496455\n",
      "Epoch: 3/4... Step: 1700... Loss: 0.212700... Val Loss: 0.456030\n",
      "Epoch: 3/4... Step: 1800... Loss: 0.173085... Val Loss: 0.419883\n",
      "Epoch: 3/4... Step: 1900... Loss: 0.138948... Val Loss: 0.428852\n",
      "Epoch: 4/4... Step: 2000... Loss: 0.170188... Val Loss: 0.407928\n",
      "Epoch: 4/4... Step: 2100... Loss: 0.330587... Val Loss: 0.540048\n",
      "Epoch: 4/4... Step: 2200... Loss: 0.222846... Val Loss: 0.489188\n",
      "Epoch: 4/4... Step: 2300... Loss: 0.120426... Val Loss: 0.478415\n",
      "Epoch: 4/4... Step: 2400... Loss: 0.161173... Val Loss: 0.462427\n",
      "Epoch: 4/4... Step: 2500... Loss: 0.328694... Val Loss: 0.450466\n",
      "Epoch: 4/4... Step: 2600... Loss: 0.272506... Val Loss: 0.479380\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "count = 0\n",
    "print_every = 100\n",
    "clip = 5\n",
    "\n",
    "net.train()\n",
    "for epoch in range(epochs):\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        count += 1\n",
    "        h = tuple([each.data for each in h])\n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs, h)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if count % print_every == 0:\n",
    "            net.eval()\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses =[]\n",
    "            for x,y in val_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                out, val_h = net(x,h)\n",
    "                val_loss = criterion(out.squeeze(), y.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "            net.train()\n",
    "            \n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "                  \"Step: {}...\".format(count),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1c3a18",
   "metadata": {},
   "source": [
    "I chose the train my model over 4 epochs, during each one I initiated the hidden state and cell state, passed the input through the model, calculated the loss and used backpropagation to update the weights. After each training loop I did a validation loop and printed the validation error to compare it with the training error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12200f21",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6c364e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss 0.47859233851174277\n",
      "Test accuracy 0.7964\n"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "n_correct = 0\n",
    "net.eval()\n",
    "h = net.init_hidden(batch_size)\n",
    "for x, y in test_loader:\n",
    "    h = tuple([each.data for each in h])\n",
    "    out, h = net(x,h)\n",
    "    test_loss = criterion(out.squeeze(), y.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    pred = torch.round(out.squeeze())\n",
    "    correct_tensor = pred.eq(y.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy())\n",
    "    n_correct += np.sum(correct)\n",
    "print(\"Test loss\", np.mean(test_losses))\n",
    "test_acc = n_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ef2665",
   "metadata": {},
   "source": [
    "In testing I followed the same steps as validation, plus I kept track of the number of correct answers my model got. Finally I printed the test loss and test accuracy which seems alright (almost 0.8)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4247bb",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1bce3399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(test_message):\n",
    "    lower = test_message.lower()\n",
    "    no_pun = \" \".join([x for x in lower if x not in punctuation])\n",
    "    test_words = no_pun.split()\n",
    "    encoded = []\n",
    "    encoded.append([vocab_to_int[w] for w in test_words])\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b3fdb692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, test_message, seq_length=200):\n",
    "    tokenized = tokenize(test_message)\n",
    "    padded = pad_features(tokenized, seq_length)\n",
    "    padded_t = torch.from_numpy(padded)\n",
    "    h = net.init_hidden(1)\n",
    "    h = tuple([each.data for each in h])\n",
    "    net.eval()\n",
    "    out, h = net(padded_t, h)\n",
    "    out = torch.round(out.squeeze())\n",
    "    if out == 1:\n",
    "        print(\"positive\")\n",
    "    else:\n",
    "        print(\"negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e7ef52",
   "metadata": {},
   "source": [
    "In the above I simply created two functions: one to preprocess the input in the same way I did before (lower case, remove punctuation and encode the words), and the second to pad the input, pass it through the model and produce an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b08bfdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "test_1 = \"It was one of the worst movies I've ever seen, I want my money back\"\n",
    "predict(net, test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "17436336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "test_1 = \"It was a great experience, absolutely delightful, one of the best movies I have ever seen\"\n",
    "predict(net, test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d015f375",
   "metadata": {},
   "source": [
    "Finally I tested my model on unseen data (two sentences that I made up) and got accurate answers !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baad78b",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this notebook I had the opportunity to experiment with lstm for sentiment analysis. My input was raw textual data (movie reviews) that I preprocessed and fed to my model. The results were as great as can be expected, I got 0.8 almost in accuracy and an error as low as 0.4 and during inference I tested my model on new sentences and got accurate answers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
